{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 1: Setup - Install Libraries and Import Dependencies**\n"
      ],
      "metadata": {
        "id": "yWTSNO3tjROd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq transformers sentence-transformers faiss-cpu langchain streamlit pyngrok PyPDF2 langchain-community\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "import io # For handling file uploads in Streamlit\n",
        "\n",
        "# Hugging Face and LangChain imports\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings # Changed from langchain.embeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.llms import HuggingFacePipeline # Changed from langchain.llms\n",
        "\n",
        "# Streamlit and Ngrok imports\n",
        "import streamlit as st # This import is mainly for type hinting/IDE, actual app runs from app.py\n",
        "from pyngrok import ngrok\n",
        "import subprocess # To run Streamlit in the background\n",
        "\n",
        "# PDF parsing\n",
        "import PyPDF2\n",
        "\n",
        "# Suppress specific warnings that might clutter output\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers')\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"Libraries installed and dependencies imported.\")\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "# Define chunking parameters\n",
        "CHUNK_SIZE = 400  # Number of tokens/characters per chunk. Adjust based on document type.\n",
        "CHUNK_OVERLAP = 40 # Overlap between chunks to maintain context.\n",
        "\n",
        "# Model names\n",
        "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2' # Recommended embedding model for good balance of speed/performance\n",
        "LLM_MODEL_NAME = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' # Small, accessible LLM for Colab free tier"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLibraries installed and dependencies imported.\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHJVlY_OjROg",
        "outputId": "9b85e8dc-c393-4697-e9af-8202c18fbf96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 2: Core RAG Functions**\n"
      ],
      "metadata": {
        "id": "oONDhGl5jROh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache the embedding model to avoid reloading on every Streamlit rerun\n",
        "@st.cache_resource\n",
        "def load_embedding_model(model_name):\n",
        "    \"\"\"Loads the HuggingFace embedding model.\"\"\"\n",
        "    print(f\"Loading embedding model: {model_name}...\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "    print(\"Embedding model loaded.\")\n",
        "    return embeddings\n",
        "\n",
        "# Cache the LLM to avoid reloading on every Streamlit rerun\n",
        "@st.cache_resource\n",
        "def load_llm(model_name, device):\n",
        "    \"\"\"Loads the LLM and sets up the HuggingFace pipeline.\"\"\"\n",
        "    print(f\"Loading tokenizer for LLM: {model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "\n",
        "    print(f\"Loading LLM: {model_name}...\")\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            trust_remote_code=True,\n",
        "        ).to(device)\n",
        "        print(\"LLM loaded successfully.\")\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1,\n",
        "            return_full_text=False,\n",
        "            pad_token_id=tokenizer.eos_token_id # Set pad_token_id to avoid warnings\n",
        "        )\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        print(\"LLM configured via HuggingFacePipeline for LangChain.\")\n",
        "        return llm\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load LLM '{model_name}'. This might be due to insufficient VRAM or model compatibility issues. Error details: {e}\")\n",
        "        class DummyLLM:\n",
        "            def __call__(self, prompt, stop=None):\n",
        "                return \"Dummy answer: LLM failed to load. Please check your LLM configuration.\"\n",
        "        return DummyLLM()\n",
        "\n",
        "# Function to process the uploaded document and create the vector store\n",
        "@st.cache_data\n",
        "def process_document(uploaded_file_content_bytes, file_extension, _embeddings_model, chunk_size, chunk_overlap):\n",
        "    \"\"\"Processes document content, chunks it, and creates a FAISS vector store.\"\"\"\n",
        "    print(\"Processing document and creating chunks...\")\n",
        "    text_content = \"\"\n",
        "    if file_extension == 'pdf':\n",
        "        try:\n",
        "            pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file_content_bytes))\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                text_content += pdf_reader.pages[page_num].extract_text()\n",
        "            print(\"Content loaded from PDF.\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error reading PDF: {e}\")\n",
        "            return None\n",
        "    elif file_extension == 'txt':\n",
        "        text_content = uploaded_file_content_bytes.decode(\"utf-8\")\n",
        "        print(\"Content loaded from TXT.\")\n",
        "    else:\n",
        "        st.error(\"Unsupported file type. Please upload a .txt or .pdf file.\")\n",
        "        return None\n",
        "\n",
        "    if not text_content.strip():\n",
        "        st.warning(\"Uploaded document is empty or could not be read.\")\n",
        "        return None\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunks = text_splitter.create_documents([text_content])\n",
        "    print(f\"Number of chunks created: {len(chunks)}\")\n",
        "\n",
        "    if not chunks:\n",
        "        st.warning(\"No chunks could be created from the document. It might be too short or unreadable.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Creating FAISS vector store...\")\n",
        "    vector_store = FAISS.from_documents(chunks, _embeddings_model)\n",
        "    print(\"FAISS vector store created successfully.\")\n",
        "    return vector_store\n",
        "\n",
        "print(\"Core RAG functions defined.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-18 08:08:53.968 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core RAG functions defined.\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqOUkC8IjROh",
        "outputId": "38794fa2-d1fa-4905-f5ce-8365ca11196f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 3: Streamlit Application Code**"
      ],
      "metadata": {
        "id": "A4266HASjROh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit_app_code = f\"\"\"\n",
        "import streamlit as st\n",
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "import io\n",
        "\n",
        "# Hugging Face and LangChain imports\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# PDF parsing\n",
        "import PyPDF2\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers')\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# --- Configuration Parameters (duplicated for app.py) ---\n",
        "CHUNK_SIZE = {CHUNK_SIZE}\n",
        "CHUNK_OVERLAP = {CHUNK_OVERLAP}\n",
        "EMBEDDING_MODEL_NAME = '{EMBEDDING_MODEL_NAME}'\n",
        "LLM_MODEL_NAME = '{LLM_MODEL_NAME}'\n",
        "\n",
        "# Determine the device to use (GPU if available, otherwise CPU)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Cache the embedding model to avoid reloading on every Streamlit rerun\n",
        "@st.cache_resource\n",
        "def load_embedding_model(model_name):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "    return embeddings\n",
        "\n",
        "# Cache the LLM to avoid reloading on every Streamlit rerun\n",
        "@st.cache_resource\n",
        "def load_llm(model_name, device):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            trust_remote_code=True,\n",
        "        ).to(device)\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1,\n",
        "            return_full_text=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        return llm\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading LLM: {{e}}. Using a dummy LLM.\")\n",
        "        class DummyLLM:\n",
        "            def __call__(self, prompt, stop=None):\n",
        "                return \"Dummy answer: LLM failed to load.\"\n",
        "        return DummyLLM()\n",
        "\n",
        "# Function to process the uploaded document and create the vector store\n",
        "@st.cache_data\n",
        "def process_document(uploaded_file_content_bytes, file_extension, _embeddings_model, chunk_size, chunk_overlap):\n",
        "    text_content = \"\"\n",
        "    if file_extension == 'pdf':\n",
        "        try:\n",
        "            pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file_content_bytes))\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                text_content += pdf_reader.pages[page_num].extract_text()\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error reading PDF: {{e}}\")\n",
        "            return None\n",
        "    elif file_extension == 'txt':\n",
        "        text_content = uploaded_file_content_bytes.decode(\"utf-8\")\n",
        "    else:\n",
        "        st.error(\"Unsupported file type. Please upload a .txt or .pdf file.\")\n",
        "        return None\n",
        "\n",
        "    if not text_content.strip():\n",
        "        st.warning(\"Uploaded document is empty or could not be read.\")\n",
        "        return None\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        add_start_index=True\n",
        "    )\n",
        "    chunks = text_splitter.create_documents([text_content])\n",
        "\n",
        "    if not chunks:\n",
        "        st.warning(\"No chunks could be created from the document. It might be too short or unreadable.\")\n",
        "        return None\n",
        "\n",
        "    vector_store = FAISS.from_documents(chunks, _embeddings_model)\n",
        "    return vector_store\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(page_title=\"RAG QA System\", layout=\"wide\")\n",
        "\n",
        "st.title(\"ðŸ“„ Mini LLM-Powered QA System (RAG)\")\n",
        "st.markdown(\"Upload a document, ask a question, and get answers powered by an open-source LLM!\")\n",
        "\n",
        "# Load models (cached)\n",
        "embeddings = load_embedding_model(EMBEDDING_MODEL_NAME)\n",
        "llm = load_llm(LLM_MODEL_NAME, DEVICE)\n",
        "\n",
        "# File uploader\n",
        "uploaded_file = st.file_uploader(\"Upload your document (.txt or .pdf)\", type=[\"txt\", \"pdf\"])\n",
        "\n",
        "vector_store = None\n",
        "if uploaded_file is not None:\n",
        "    file_extension = uploaded_file.name.split('.')[-1].lower()\n",
        "    # Process document and create vector store\n",
        "    with st.spinner(\"Processing document and building knowledge base...\"):\n",
        "        # Pass the file content as bytes\n",
        "        vector_store = process_document(uploaded_file.read(), file_extension, embeddings, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "    if vector_store:\n",
        "        st.success(\"Document processed and knowledge base ready!\")\n",
        "    else:\n",
        "        st.error(\"Failed to process document. Please check the file content and type.\")\n",
        "\n",
        "# Query interface\n",
        "if vector_store:\n",
        "    st.subheader(\"Ask a Question\")\n",
        "    query = st.text_area(\"Enter your question here:\", height=100)\n",
        "\n",
        "    if st.button(\"Get Answer\"):\n",
        "        if query:\n",
        "            with st.spinner(\"Retrieving answer...\"):\n",
        "                retriever = vector_store.as_retriever(search_kwargs={{\"k\": 3}})\n",
        "                qa_chain = RetrievalQA.from_chain_type(\n",
        "                    llm=llm,\n",
        "                    chain_type=\"stuff\",\n",
        "                    retriever=retriever,\n",
        "                    return_source_documents=True\n",
        "                )\n",
        "                response = qa_chain({{\"query\": query}})\n",
        "\n",
        "            st.subheader(\"Generated Answer:\")\n",
        "            st.write(response[\"result\"])\n",
        "\n",
        "            st.subheader(\"Source Documents:\")\n",
        "            if response[\"source_documents\"]:\n",
        "                for i, doc in enumerate(response[\"source_documents\"]):\n",
        "                    st.markdown(f\"**Chunk {{i+1}} (Source Index: {{doc.metadata.get('start_index', 'N/A')}}):**\")\n",
        "                    st.info(doc.page_content)\n",
        "            else:\n",
        "                st.write(\"No relevant source documents found.\")\n",
        "        else:\n",
        "            st.warning(\"Please enter a question.\")\n",
        "else:\n",
        "    st.info(\"Please upload a document to get started.\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"Built for Wundrsight SWE Intern Technical Assignment.\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(streamlit_app_code)\n",
        "\n",
        "print(\"Streamlit app code written to 'app.py'.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app code written to 'app.py'.\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5a3mmyljROi",
        "outputId": "6cd14bff-f69f-4773-bca9-22192b012f1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Cell 4: Run Streamlit App with Ngrok**"
      ],
      "metadata": {
        "id": "f6GRVaCDjROi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_AUTH_TOKEN = \"2z2QXqdZspDDxtlmAXuITrNtfVe_4SS7Juw42gtFAQG3TTDzn\"\n",
        "\n",
        "if NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTH_TOKEN\":\n",
        "    print(\"\\nWARNING: Please replace 'YOUR_NGROK_AUTH_TOKEN' in the code with your actual ngrok authentication token.\")\n",
        "    print(\"You can get it from https://dashboard.ngrok.com/get-started/your-authtoken after signing up.\")\n",
        "else:\n",
        "    # Authenticate ngrok\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "    print(\"Ngrok authentication token set.\")\n",
        "\n",
        "    # Kill any running ngrok processes to avoid conflicts\n",
        "    # Use pkill for a more forceful kill\n",
        "    !pkill ngrok\n",
        "\n",
        "    # Start ngrok tunnel for Streamlit (port 8501 is default for Streamlit)\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(f\"Streamlit App Tunnel URL: {public_url}\")\n",
        "    print(\"Click the URL above to open your Streamlit app in a new tab.\")\n",
        "\n",
        "    # Run the Streamlit app in the background\n",
        "    # We use subprocess.Popen to keep the Colab cell running and display the output\n",
        "    try:\n",
        "        # Start Streamlit in a separate process\n",
        "        # --server.port 8501: Ensures Streamlit runs on the port ngrok is tunneling\n",
        "        # --server.headless true: Prevents Streamlit from trying to open a browser on the Colab server\n",
        "        process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        # Give it a moment to start\n",
        "        import time\n",
        "        time.sleep(5)\n",
        "        print(\"\\nStreamlit app is running in the background. Check the ngrok URL above.\")\n",
        "        print(\"You can monitor Streamlit logs by running `!cat ~/.streamlit/logs.txt` in a new cell if needed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error starting Streamlit: {e}\")\n",
        "        print(\"Please ensure 'app.py' was created successfully in the previous cell and ngrok token is valid.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ngrok authentication token set.\n",
            "Streamlit App Tunnel URL: NgrokTunnel: \"https://a44f212ffc36.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "Click the URL above to open your Streamlit app in a new tab.\n",
            "\n",
            "Streamlit app is running in the background. Check the ngrok URL above.\n",
            "You can monitor Streamlit logs by running `!cat ~/.streamlit/logs.txt` in a new cell if needed.\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UlNI2PrjROi",
        "outputId": "a254e090-5418-48d6-ef42-6824efe060fd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qc6pHxCXkTVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}